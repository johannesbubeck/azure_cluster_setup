# Before we can install using the current repo we need to init stuff...
sudo apt-get update 

# Install make
sudo apt install make

# Install FORTRAN
sudo apt install gfortran -y

curl -Lo- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | sudo gpg --dearmor -o /usr/share/keyrings/oneapi-archive-keyring.gpg~g --dearmor -o /usr/share/keyrings/oneapi-archive-keyring.gpg
sudo tee /etc/apt/sources.list.d/oneAPI.list <<< "deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main"
sudo apt update
sudo apt install intel-oneapi-compiler-fortran
ulimit -s unlimited
set stacksize unlimited
echo 'source /opt/intel/oneapi/setvars.sh > /dev/null' >> ~/.bashrc 
source ~/.bashrc

# Install JULIA
wget https://julialang-s3.julialang.org/bin/linux/x64/1.8/julia-1.8.2-linux-x86_64.tar.gz
tar zxvf julia-1.8.2-linux-x86_64.tar.gz
rm julia-1.8.2-linux-x86_64.tar.gz 
echo 'export PATH="$PATH:/home/$USER/julia-1.8.2/bin"' >> ~/.bashrc 
source ~/.bashrc

#For resource scheduling we use SLURM
sudo apt install -y slurm-wlm slurm-wlm-doc
# To check resources -> put this information into the config file!
lscpu
# To check the path to the config file... (usually /etc/slurm/slurm.conf)
cat /lib/systemd/system/slurmctld.service
cat /lib/systemd/system/slurmd.service

sudo chmod 777 /etc/slurm/

sudo cat << EOF > /etc/slurm/slurm.conf
# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ClusterName=annikacluster
SlurmctldHost=$HOSTNAME
MpiDefault=none
ProctrackType=proctrack/linuxproc
ReturnToService=1
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurm-llnl
SlurmUser=azureuser
StateSaveLocation=/var/spool/slurm-llnl
SwitchType=switch/none
TaskPlugin=task/affinity
#
# TIMERS
InactiveLimit=0
KillWait=30
MinJobAge=300
SlurmctldTimeout=120
SlurmdTimeout=300
Waittime=0
#
# SCHEDULING
SchedulerType=sched/builtin
SelectType=select/cons_res
SelectTypeParameters=CR_Core_Memory
#
# LOGGING AND ACCOUNTING
AccountingStorageType=accounting_storage/none
JobCompType=jobcomp/filetxt
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/linux
SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurmd.log
#
# COMPUTE NODES
NodeName=$HOSTNAME CPUs=32 RealMemory=200000 Sockets=1 CoresPerSocket=16 ThreadsPerCore=2 State=UNKNOWN
PartitionName=debug Nodes=ALL Default=YES MaxTime=INFINITE State=UP

EOF

# This is probably not necessary
# sudo apt install -y mailutils

sudo mkdir -p /var/spool/slurm-llnl/
sudo touch /var/log/slurm_jobcomp.log
sudo chown $USER:$USER /var/spool/slurm-llnl /var/log/slurm_jobcomp.log

sudo service munge start
sudo ls -l /etc/munge/munge.key
munge -n | unmunge

# First try
sudo service slurmctld start
sudo service slurmd start

cat /etc/slurm/slurm.conf
cat /var/log/slurmctld.log
cat /var/log/slurmd.log

# Here we need to have one idle node
sinfo
# Should be empty probably
squeue

# Second try if necessary
sudo service slurmctld start
sudo service slurmd start

# Here we need to have one idle node
sinfo
# Should be empty probably
squeue

# Finally, we can use
srun --usage
sbatch --usage
scancel --usage
#cat slurm-<JOB_ID>.out
#scancel <JOB_ID>


# How to run the example file:
# cd model_1
# srun --cpus-per-task=32 bash ./run_mod.sh
# sbatch --cpus-per-task=32 ./run_mod.sh
